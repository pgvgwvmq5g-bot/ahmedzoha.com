---
// blog/index.astro - Blog page with static content
import BaseLayout from '../../layouts/BaseLayout.astro';

// Static Blog Posts - Add new posts here
const blogPosts = [
  {
    slug: 'thinking-aloud-about-agentic-ai',
    title: 'Thinking Aloud About Agentic AI',
    excerpt: 'Agentic AI breaks the pattern. These systems do not just observe and advise. They decide and act, often before a human realises there is anything to respond to.',
    content: `
      <p>I still remember the moment I realised something fundamental had changed.</p>

      <p>We were testing an automated network control system. Nothing had failed. Dashboards looked normal. And yet the system quietly rerouted traffic away from a link that, to everyone in the room, appeared healthy. About twenty minutes later, that same link degraded sharply. The AI had acted on something we had not yet seen.</p>

      <p>That was the first time I felt I was no longer working with a tool that simply analysed the world. I was working with something that formed expectations about it.</p>

      <p>I have spent much of my career working at the intersection of AI and critical infrastructure: telecoms, energy systems, and security-sensitive platforms where mistakes scale quickly and forgiveness is limited. For years, AI in these environments was reactive. It detected anomalies, raised alerts, and waited for humans to decide.</p>

      <p>Agentic AI breaks that pattern. These systems do not just observe and advise. They decide and act, often before a human realises there is anything to respond to.</p>

      <h2>From Reacting to Anticipating</h2>

      <p>In telecoms, agentic systems now reallocate spectrum or spin up network functions ahead of congestion. In energy systems, they forecast demand or renewable variability and adjust control strategies early. In security, they isolate assets or alter access pathways based on inferred intent rather than confirmed attacks.</p>

      <p>The value is obvious. Humans cannot operate at this speed or scale. But something subtle happens when systems move from reacting to anticipating. They stop acting purely on facts and start acting on beliefs about the future.</p>

      <p>And beliefs can be wrong.</p>

      <p>When an AI throttles traffic because it predicts congestion that never arrives, who absorbs that cost? When an energy system sheds load preemptively, how do we explain that decision to those affected? When a security agent blocks access because behaviour looks suspicious rather than malicious, how easily can that decision be unwound?</p>

      <p>These questions became real for me during the MIMER project.</p>

      <h2>What MIMER Forced Us to Confront</h2>

      <p>MIMER sits at the boundary between identity verification, security, and critical digital infrastructure. The problem we were tackling was simple to state but hard to solve. Remote identity verification systems are increasingly vulnerable to sophisticated spoofing, including deepfake-driven impersonation. These systems underpin access to financial services, telecom onboarding, and essential digital platforms.</p>

      <p>Static rules do not survive long in adversarial environments. Attackers adapt too quickly. So we explored adaptive approaches, including agentic components that could observe evolving attack patterns, reason about intent, and adjust verification strategies dynamically.</p>

      <p>What surprised me was how quickly uncertainty became operational. Confidence scores that work well as decision support become fragile when agents act on them autonomously. Humans can contextualise borderline cases. Agents tend to treat thresholds as truths unless carefully constrained.</p>

      <p>We also learned that explainability degrades as agency increases. When an agent chains perception, memory, and action, reconstructing why a decision was made becomes genuinely difficult. In regulated environments, saying "the model decided" is not an answer.</p>

      <p>MIMER made one thing clear to me: Agentic AI is not primarily a modelling challenge. It is a governance challenge that happens to involve models.</p>

      <h2>The Trust Problem</h2>

      <p>Trust behaves differently in critical infrastructure than it does in education or consumer technology.</p>

      <p>In education, we worry people trust AI too much. In critical systems, engineers often trust it too little. That instinct is healthy, until it undermines the very benefits autonomy offers.</p>

      <p>During MIMER trials, some operators wanted to veto every decision, reducing the agent to a sophisticated alarm. Others were tempted to step back entirely because early performance looked strong. Neither felt safe.</p>

      <p>What we lack are good ways to calibrate trust over time. Not blind faith and not permanent scepticism, but situational trust that adapts to context, risk, and uncertainty. We talk extensively about accuracy and robustness, but far less about how humans actually experience supervising autonomous systems day after day.</p>

      <h2>The Accountability Gap</h2>

      <p>This is where the conversation gets uncomfortable.</p>

      <p>When an AI agent makes a decision in a telecom network or energy grid, who is responsible? The engineer who deployed it? The organisation that configured it? The vendor that supplied the model? The regulator who approved its use?</p>

      <p>Traditional systems had clearer accountability chains. Agentic systems do not fail cleanly. They drift, cascade, and compound. Without deliberate design for traceability, logging, and meaningful human override, accountability simply dissolves.</p>

      <p>Security communities are beginning to grapple with this. Recent work on agentic AI threat modelling highlights risks such as cascading decision failures, untraceable actions, and overwhelmed human oversight in multi-agent systems. These risks are not theoretical. They emerge naturally when autonomy scales faster than governance.</p>

      <h2>What Keeps Me Awake</h2>

      <p>I worry less about AI making mistakes than about systems failing quietly through chains of reasonable-looking decisions. By the time humans notice, the system may already be far from its original intent.</p>

      <p>I worry about boundary erosion. Each automated decision subtly shifts what we consider acceptable to delegate. Over time, no one remembers why certain decisions were once considered too important to automate.</p>

      <p>And I worry about human disengagement. Supervising autonomy is cognitively demanding. The more capable these systems become, the harder it is for humans to stay meaningfully involved. We do not yet design agentic systems that respect human attention and fatigue in safety-critical contexts.</p>

      <h2>The Human Element</h2>

      <p>Despite these concerns, I am not pessimistic.</p>

      <p>Agentic AI can make critical infrastructure more resilient and adaptive, reducing brittle, reactive decision-making. But only if trust, accountability, and human judgment are treated as first-class design problems.</p>

      <p>An AI agent can decide how to reroute traffic. It should not decide whose connectivity matters more in a crisis without human involvement. An agent can detect identity risk patterns. It should not redefine fairness thresholds on its own.</p>

      <p>The boundary between human and machine judgment is not fixed, and it should not be. But it must be intentional.</p>

      <h2>Where This Leaves Us</h2>

      <p>MIMER did not give me neat answers. What it gave me was clarity about the questions we should be asking more openly.</p>

      <p>Agentic AI is already reshaping telecoms, energy, and security. The real question is not whether these systems will act on our behalf.</p>

      <p>It is how much agency we are willing to delegate before we are no longer entirely sure who is still in charge.</p>

      <p>I do not think we should rush to answer that. But I do think we need to keep asking it, honestly, and before the systems decide for us.</p>
    `,
    date: 'December 5, 2025',
    dateSort: new Date('2025-12-05'),
    readTime: '10 min read',
    category: 'AI & Infrastructure',
    featuredImage: '/images/blog/agentic_ai.png',
    author: {
      name: 'Dr Ahmed Zoha',
      avatar: '/images/profile.jpg',
    },
  },
  {
    slug: 'ai-in-higher-education',
    title: 'The Conversation We\'re Not Having About AI in Higher Education',
    excerpt: 'We\'re so caught up in debates about academic integrity that we\'re missing something fundamental about how learning actually happens. As someone who\'s championed dialogic approaches to teaching, I explore whether AI might open up new spaces for rich, generative dialogue or shut those spaces down entirely.',
    content: `
      <p>I've been teaching long enough to recognize when something is genuinely transformative rather than just the latest shiny object in edtech. When I first encountered generative AI in my classroom through student submissions, my reaction was somewhere between fascination and mild panic.</p>

      <p>But here's what's been nagging at me: we're so caught up in debates about academic integrity that we're missing something fundamental about how learning actually happens. As someone who's championed dialogic approaches to teaching for years, I keep wondering whether AI might actually open up new spaces for rich, generative dialogue or whether it's going to shut those spaces down entirely.</p>

      <h2>What Gets Lost in Translation</h2>

      <p>Dialogic teaching isn't just about discussion. It's about creating conditions where thinking happens through conversation, where ideas are tested, reshaped, and built upon collectively. It's messy. It requires students to articulate half-formed thoughts, to stumble, to hear themselves think out loud.</p>

      <p>The best moments in my seminars happen when a student says, "Wait, I'm not sure this makes sense, but..." and then the whole room leans in. That uncertainty, that willingness to think publicly, is where deep learning lives.</p>

      <p>So when AI entered our classrooms, my first question wasn't really about plagiarism. It was: what happens to this dialogic space?</p>

      <h2>The Unexpected Bits</h2>

      <p>I've been watching closely, and it's more complicated than I expected. A recent study from the University of Liverpool looked at architecture students using AI image generators. Students who crafted more conceptually rich prompts produced more compelling visual outputs. Nothing groundbreaking there.</p>

      <p>But what caught my attention was this: students reported high levels of creative support, particularly in exploration. They weren't passively consuming AI outputs. They were iterating through language to refine their design intentions. It looked suspiciously like dialogue.</p>

      <p>This challenged my initial assumption that AI would short-circuit learning. Instead, students were learning a new literacy that required them to articulate their thinking with both precision and creativity.</p>

      <h2>Where I'm Worried</h2>

      <p>The Liverpool study found that collaboration and immersion scores were notably lower than other dimensions. That matters. Learning isn't just about producing good outputs. It's about the struggle, the shared frustration of working through difficult ideas with other people.</p>

      <p>I'm also worried about student voice. In dialogic learning, we talk about students developing their unique perspective. But when you're constantly negotiating with an AI trained on millions of texts, whose voice is actually emerging? There's a real risk that students start sounding like slightly customized versions of a language model.</p>

      <p>And then there's intellectual dependency. If students outsource the hard cognitive work of drafting to AI, are they actually developing as thinkers? Writing isn't just about producing text. It's about wrestling with ideas until they make sense.</p>

      <h2>What I'm Trying Instead</h2>

      <p>I've started experimenting. Instead of banning AI, I ask students to use it as a thinking partner, then bring that conversation into our classroom dialogue. "Show me what the AI suggested, then tell me where it's wrong or incomplete."</p>

      <p>The results have been surprising. Students who normally stay quiet seem more confident when they've rehearsed ideas with AI first. Others use AI to generate provocative counterarguments that push discussion in unexpected directions. One student told me it was like "having a really patient but occasionally clueless study partner who helps you figure out what you actually think."</p>

      <p>That last bit is crucial: what you actually think. The AI doesn't replace thinking. It becomes a tool for externalizing and examining it.</p>

      <h2>Putting Theory into Practice: The LEAP Project</h2>

      <p>These aren't just theoretical concerns for me. I'm currently leading <a href="/projects/leap/" class="text-accent-yellow hover:underline">LEAP (Learning Enhancement through Adaptive Pathways)</a>, a two-year Knowledge Transfer Partnership funded by Innovate UK. We're developing an AI platform specifically designed to support teachers and enhance student learning through dialogue-driven thinking.</p>

      <p>The goal isn't to replace human teaching but to amplify educator expertise. LEAP uses adaptive pathways to create spaces where AI acts as a scaffold for deeper thinking, not a shortcut around it. It's about designing AI tools that preserve and enhance the dialogic spaces I've been talking about, rather than collapsing them.</p>

      <p>If you're interested in how we're approaching this challenge practically, <a href="/projects/leap/" class="text-accent-yellow hover:underline">you can learn more about the LEAP project here</a>.</p>

      <h2>What Needs to Change</h2>

      <p>Most universities have responded to AI with policies about when it can be used, focusing on detection and prevention. That's treating AI as a problem to manage rather than a shift that needs thoughtful integration.</p>

      <p>We need to redesign assessments around process, not just product. If students can document their dialogue with AI, showing how they tested ideas and rejected suggestions, that becomes evidence of learning.</p>

      <p>We should teach prompt engineering as a rhetorical practice. How do you articulate a question well? How do you push back against an answer that seems plausible but feels wrong? These are exactly the skills we've always wanted students to develop.</p>

      <p>And critically, we need to preserve spaces for human-only dialogue. Not because AI is bad, but because certain kinds of learning, particularly those involving emotion and ethics, require human presence and vulnerability.</p>

      <h2>What Keeps Me Up</h2>

      <p>Can students develop genuine intellectual autonomy if they're always in dialogue with a tool that seems to know everything? What happens to productive confusion if AI is always there to resolve ambiguity?</p>

      <p>And perhaps most importantly: are we asking students to engage in dialogue with AI, or are we asking AI to simulate the dialogue that should be happening between humans?</p>

      <h2>No Easy Answers</h2>

      <p>Those architecture students had 45 minutes to design a residential space using only text prompts. The researchers found that richer, more varied language in prompts correlated with better outcomes. But the real insight was about what happened between intention and output, in that iterative dance of articulation and revision.</p>

      <p>That's where learning happens. That's what we need to protect as we navigate this shift.</p>

      <p>I'm not arguing for or against AI in education. That debate feels beside the point. AI is here. The question is whether we can shape that integration in ways that deepen rather than diminish learning.</p>

      <p>My hope is that we can. But it requires us to be thoughtful, experimental, and willing to admit when we're unsure. It requires treating AI not as an answer but as a new participant in an ongoing conversation about what education is for.</p>

      <p>What's your experience been? I'm genuinely curious how other educators are navigating this.</p>
    `,
    date: 'December 10, 2025',
    dateSort: new Date('2025-12-10'),
    readTime: '8 min read',
    category: 'AI & Education',
    featuredImage: '/images/blog/academia_ai.png',
    author: {
      name: 'Dr Ahmed Zoha',
      avatar: '/images/profile.jpg',
    },
  },
  {
    slug: 'privacy-paradox-federated-learning',
    title: 'The Privacy Paradox: Why Federated Learning Is Not Enough',
    excerpt: 'I remember the moment when my confidence in federated learning began to slip. From the gradients alone, those supposedly harmless updates, they reconstructed recognisable features from the training data.',
    content: `
      <h2>The moment my confidence slipped</h2>

      <p>I remember the moment when my confidence in federated learning began to slip.</p>

      <p>We were testing a privacy preserving system for detecting deepfake attacks in remote identity verification. On paper, everything looked right. Sensitive data stayed local. Only model updates were shared. Noise was added to protect privacy. It was the story I had told many times before, to partners, funders, and students.</p>

      <p>Then a colleague showed something that made the room go quiet. From the gradients alone, those supposedly harmless updates, they reconstructed recognisable features from the training data. Not perfectly, but clearly enough to be unsettling.</p>

      <p>That was the moment I realised federated learning was not the privacy solution I had believed it to be. It was progress, yes. But it also created new risks we were still learning how to see.</p>

      <h2>The elegance that drew us in</h2>

      <p>Federated learning is seductive because it appears to resolve a contradiction. We want powerful models, but we do not want to centralise sensitive data. So we leave the data where it is and share only what the model learns.</p>

      <p>For a long time, I repeated this logic without hesitation. The mathematics is elegant. Differential privacy gives us bounds. Secure aggregation hides individual contributions. Encryption lets us compute without seeing.</p>

      <p>The problem is that mathematics lives in a clean world. Deployed systems do not.</p>

      <h2>Where the promise starts to fracture</h2>

      <p>The core promise of federated learning rests on one fragile assumption. That model updates cannot be reverse engineered in meaningful ways.</p>

      <p>In reality, they can.</p>

      <p>Gradients can leak more than intuition suggests. Images can be partially reconstructed. Participation can be inferred. Sensitive attributes can be teased out even when raw data never moves.</p>

      <p>When we pushed our own systems harder, the same pattern appeared. Privacy protections reduced leakage, but they also reduced performance. More noise meant weaker detection. Stronger guarantees meant poorer utility.</p>

      <p>We were forced into an uncomfortable trade off. Protect privacy too weakly and risk exposure. Protect it too strongly and undermine the system's purpose.</p>

      <h2>Theory meets deployed reality</h2>

      <p>In conversations with engineers who have deployed federated systems, the same theme comes up again and again. Every privacy mechanism adds complexity. Every layer introduces new assumptions. And every assumption can break.</p>

      <p>Privacy guarantees rarely fail because the maths is wrong. They fail because real systems are assembled from many moving parts, often bolted onto existing infrastructure under time and resource constraints.</p>

      <p>A small oversight in one component can quietly undo strong guarantees elsewhere.</p>

      <h2>Privacy as an adversarial problem</h2>

      <p>This is not just a technical challenge. It is an adversarial one.</p>

      <p>Federated systems attract attackers precisely because they promise privacy. Gradient leakage, membership inference, model inversion, and poisoning attacks all exist because someone is actively trying to extract value from what is meant to be hidden.</p>

      <p>Each defence invites a counter attack. Each fix shifts the problem rather than eliminating it. We are in an arms race, and there is no stable end point.</p>

      <h2>Why governance matters more than algorithms</h2>

      <p>This leads to a conclusion I resisted for a long time. Federated learning's hardest problems are not algorithmic. They are governance problems.</p>

      <p>Who decides how much privacy loss is acceptable. Who verifies that protections are implemented correctly. Who is accountable when guarantees silently erode over time.</p>

      <p>Federated learning distributes computation, but it does not remove the need for trust. It reshapes trust into a web of dependencies that is harder to reason about and harder to manage.</p>

      <h2>What this means for critical systems</h2>

      <p>For critical infrastructure, the implications are serious.</p>

      <p>In telecoms, leaked updates can reveal behavioural patterns. In healthcare, they can expose participation in sensitive studies. In finance, they can hint at individual transaction behaviour.</p>

      <p>These failures are subtle. There is no obvious breach. No database leaked online. Just gradual erosion of privacy that is difficult to detect and even harder to explain.</p>

      <p>This is the paradox. Federated learning can make privacy failures quieter and more opaque than centralised systems ever were.</p>

      <h2>The human side of privacy</h2>

      <p>What troubles me most is not a technical flaw, but a human one.</p>

      <p>People do not distinguish between data staying local and inference happening remotely. From their perspective, inference still feels like surveillance. A system that meets formal definitions but feels invasive has already failed.</p>

      <p>No amount of mathematical precision resolves that gap on its own. Trust requires transparency, communication, and honesty about trade offs.</p>

      <h2>Where this leaves us</h2>

      <p>Federated learning remains useful. But we need to stop treating it as a privacy solution. It is a privacy improving technique that works only under carefully defined conditions and only when paired with other safeguards.</p>

      <p>It can reduce risk. It cannot eliminate it.</p>

      <p>It can shift trust. It cannot remove the need for it.</p>

      <p>If we want systems that are genuinely trustworthy, we need governance frameworks that match the technical complexity of what we are building, and humility about the limits of our tools.</p>

      <h2>The question that matters</h2>

      <p>The real question is not whether federated learning preserves privacy.</p>

      <p>It is whether we are prepared to recognise when it does not, and what we do when that happens.</p>

      <p>That gap, between what works in theory and what we can trust in practice, is where the real work begins.</p>
    `,
    date: 'December 12, 2025',
    dateSort: new Date('2025-12-12'),
    readTime: '9 min read',
    category: 'AI & Privacy',
    featuredImage: '/images/blog/fedai_bp.png',
    author: {
      name: 'Dr Ahmed Zoha',
      avatar: '/images/profile.jpg',
    },
  },
  {
    slug: 'code-switching-problem-multilingual-ai',
    title: 'The Code-Switching Problem: Why Multilingual AI Is Harder Than It Looks',
    excerpt: 'I remember sitting in a meeting room in Taxila, listening to our Pakistani banking partners discuss their customer verification systems. The conversation flowed naturally between Urdu, English, and occasional Hindi phrases. Then one of the bankers asked: "Can your AI understand this?"',
    content: ``,
    date: 'December 13, 2025',
    dateSort: new Date('2025-12-13'),
    readTime: '11 min read',
    category: 'AI & Language',
    featuredImage: '/images/blog/codeswitching.png',
    author: {
      name: 'Dr Ahmed Zoha',
      avatar: '/images/profile.jpg',
    },
  },
];

// Sort posts by date (newest first)
const posts = blogPosts.sort((a, b) => b.dateSort - a.dateSort);

// Define featured post slugs (always keep these as featured)
const featuredSlugs = ['thinking-aloud-about-agentic-ai', 'ai-in-higher-education'];

// Separate featured posts and remaining posts
const featuredPosts = posts.filter(p => featuredSlugs.includes(p.slug));
const remainingPosts = posts.filter(p => !featuredSlugs.includes(p.slug));

// Get unique categories
const categories = [...new Set(posts.map(p => p.category))].filter(Boolean);

const hasPosts = posts.length > 0;
---

<BaseLayout title="Blog / Insights" description="Thoughts, tutorials, and insights on AI and technology">

  <section class="pt-8 lg:pt-16">
    <!-- Page Header -->
    <div class="mb-12">
      <h1 class="text-4xl lg:text-5xl font-light mb-4">
        Blog / <span class="text-accent-yellow">Insights</span>
      </h1>
      <p class="text-text-secondary max-w-2xl">
        Thoughts, tutorials, and perspectives on artificial intelligence,
        research, and technology.
      </p>
    </div>

    {hasPosts ? (
      <>
        <!-- Featured Posts -->
        {featuredPosts.length > 0 && (
          <div class="mb-12">
            <h2 class="text-xl font-medium mb-6">Featured Articles</h2>
            <div class="grid md:grid-cols-2 gap-6">
              {featuredPosts.map((post) => (
                <a
                  href={`/blog/${post.slug}/`}
                  class="group block bg-dark-700 rounded-2xl overflow-hidden border border-dark-500/30
                         hover:border-accent-yellow/30 transition-all"
                >
                  <div class="aspect-[16/10] bg-dark-600 relative overflow-hidden">
                    {post.featuredImage ? (
                      <img
                        src={post.featuredImage}
                        alt={post.title}
                        class="w-full h-full object-cover opacity-80 group-hover:opacity-100
                               group-hover:scale-105 transition-all duration-500"
                      />
                    ) : (
                      <div class="w-full h-full flex items-center justify-center text-text-muted">
                        <svg class="w-16 h-16 opacity-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="1" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />
                        </svg>
                      </div>
                    )}
                    <div class="absolute inset-0 bg-gradient-to-t from-dark-700 via-transparent to-transparent"></div>
                  </div>
                  <div class="p-6">
                    <div class="flex items-center gap-3 mb-3 flex-wrap">
                      {post.category && (
                        <span class="px-2 py-1 bg-accent-yellow/10 text-accent-yellow rounded text-xs">
                          {post.category}
                        </span>
                      )}
                      <span class="text-sm text-text-muted">{post.date}</span>
                      <span class="text-sm text-text-muted">• {post.readTime}</span>
                    </div>
                    <h3 class="font-medium text-lg mb-3 group-hover:text-accent-yellow transition-colors">
                      {post.title}
                    </h3>
                    <p class="text-sm text-text-secondary line-clamp-2">
                      {post.excerpt}
                    </p>
                    <span class="inline-block mt-4 text-sm text-accent-yellow opacity-0
                                group-hover:opacity-100 transition-opacity">
                      Read article →
                    </span>
                  </div>
                </a>
              ))}
            </div>
          </div>
        )}

        <!-- Categories -->
        {categories.length > 0 && (
          <div class="mb-8 flex gap-2 flex-wrap">
            <button class="category-tab px-4 py-2 rounded-lg text-sm bg-accent-yellow text-dark-900 transition-all"
                    data-category="All">
              All
            </button>
            {categories.map((cat) => (
              <button
                class="category-tab px-4 py-2 rounded-lg text-sm bg-dark-700 text-text-secondary hover:bg-dark-600 transition-all"
                data-category={cat}
              >
                {cat}
              </button>
            ))}
          </div>
        )}

        <!-- All Posts -->
        {remainingPosts.length > 0 && (
          <div class="mb-16">
            <div class="space-y-6" id="posts-container">
              {remainingPosts.map((post) => (
                <a
                  href={`/blog/${post.slug}/`}
                  class="blog-post flex flex-col md:flex-row gap-6 p-6 bg-dark-700 rounded-xl
                         border border-dark-500/30 hover:border-accent-yellow/30
                         hover:bg-dark-600 transition-all group"
                  data-category={post.category}
                >
                  <div class="w-full md:w-48 h-32 bg-dark-600 rounded-lg flex-shrink-0 overflow-hidden">
                    {post.featuredImage ? (
                      <img
                        src={post.featuredImage}
                        alt={post.title}
                        class="w-full h-full object-cover group-hover:scale-105 transition-transform duration-500"
                      />
                    ) : (
                      <div class="w-full h-full flex items-center justify-center text-text-muted">
                        <svg class="w-8 h-8 opacity-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="1" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />
                        </svg>
                      </div>
                    )}
                  </div>
                  <div class="flex-1">
                    <div class="flex items-center gap-3 mb-2 flex-wrap">
                      {post.category && (
                        <span class="px-2 py-0.5 bg-dark-600 rounded text-xs text-text-muted">
                          {post.category}
                        </span>
                      )}
                      <span class="text-sm text-text-muted">{post.date}</span>
                      <span class="text-sm text-text-muted">• {post.readTime}</span>
                    </div>
                    <h3 class="font-medium text-lg mb-2 group-hover:text-accent-yellow transition-colors">
                      {post.title}
                    </h3>
                    <p class="text-sm text-text-secondary line-clamp-2">
                      {post.excerpt}
                    </p>
                  </div>
                </a>
              ))}
            </div>
          </div>
        )}
      </>
    ) : (
      <!-- No Posts Yet -->
      <div class="bg-dark-700 rounded-2xl p-12 border border-dark-500/30 text-center mb-16">
        <div class="w-16 h-16 bg-dark-600 rounded-full flex items-center justify-center mx-auto mb-6">
          <svg class="w-8 h-8 text-accent-yellow" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.232 5.232l3.536 3.536m-2.036-5.036a2.5 2.5 0 113.536 3.536L6.5 21.036H3v-3.572L16.732 3.732z" />
          </svg>
        </div>
        <h3 class="text-xl font-medium mb-3">No Blog Posts Yet</h3>
        <p class="text-text-secondary mb-6 max-w-md mx-auto">
          Blog posts will appear here once added.
        </p>
      </div>
    )}

    <!-- Newsletter -->
    <div class="bg-dark-700 rounded-2xl p-8 border border-dark-500/30 text-center">
      <h3 class="text-xl font-medium mb-3">Subscribe to My Newsletter</h3>
      <p class="text-text-secondary mb-6 max-w-xl mx-auto">
        Get notified when I publish new articles. No spam, unsubscribe anytime.
      </p>
      <form class="flex flex-col sm:flex-row gap-3 max-w-md mx-auto">
        <input
          type="email"
          placeholder="Enter your email"
          class="flex-1 px-4 py-3 bg-dark-600 rounded-lg border border-dark-500
                 focus:border-accent-yellow focus:outline-none text-sm"
        />
        <button
          type="submit"
          class="px-6 py-3 bg-accent-yellow text-dark-900 font-medium rounded-lg
                 hover:bg-accent-yellow/90 transition-colors whitespace-nowrap"
        >
          Subscribe
        </button>
      </form>
    </div>

  </section>

  <!-- Category Filtering Script -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const tabs = document.querySelectorAll('.category-tab');
      const blogPosts = document.querySelectorAll('.blog-post');

      tabs.forEach(tab => {
        tab.addEventListener('click', () => {
          const selectedCategory = tab.getAttribute('data-category');

          // Update active tab styling
          tabs.forEach(t => {
            if (t === tab) {
              t.classList.remove('bg-dark-700', 'text-text-secondary');
              t.classList.add('bg-accent-yellow', 'text-dark-900');
            } else {
              t.classList.remove('bg-accent-yellow', 'text-dark-900');
              t.classList.add('bg-dark-700', 'text-text-secondary');
            }
          });

          // Filter posts
          blogPosts.forEach(post => {
            const postCategory = post.getAttribute('data-category');
            if (selectedCategory === 'All' || postCategory === selectedCategory) {
              post.style.display = 'flex';
            } else {
              post.style.display = 'none';
            }
          });
        });
      });
    });
  </script>

</BaseLayout>
